# -*- coding: utf-8 -*-
"""YES (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xUdItW4fmTuHEKKq0CRZbL1GDpDhGFH_
"""

#LLM

!pip install -U transformers

from huggingface_hub import login
login(new_session=False)

import csv
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch


model_name = "mistralai/Mistral-7B-Instruct-v0.2"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16
)

generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

def build_prompt(question, answer):
    return f"""You are a sentiment classification system. Your task is to strictly classify the sentiment of the following answer as either "positive" or "negative".

You must respond with **only one word**, either "positive" or "negative". Do not include explanations, punctuation, or any other text.

Do not say "neutral", "uncertain", or any other word.
Q: {question}
A: {answer}


Sentiment:"""

input_file = "LLM.tsv"
output_file = "mistral_sentiment.tsv"


with open(input_file, "r", encoding="utf-8") as fin, open(output_file, "w", encoding="utf-8", newline='') as fout:
    reader = csv.DictReader(fin, delimiter="\t")
    fieldnames = reader.fieldnames + ["predicted_sentiment_mistral"]
    writer = csv.DictWriter(fout, fieldnames=fieldnames, delimiter="\t")
    writer.writeheader()

    for row in reader:
        question = row["question"]
        answer = row["answer"]
        prompt = build_prompt(question, answer)

        result = generator(prompt, max_new_tokens=1, do_sample=False)
        output_text = result[0]["generated_text"]
        sentiment = output_text.split("Sentiment:")[-1].strip().split("\n")[0].lower()

        row["predicted_sentiment_mistral"] = sentiment
        writer.writerow(row)

        print(f"[{row['id']}] Mistral sentiment: {sentiment}")



types = ["Conditional", "Sarcastic", "Ambiguous", "Contrastive"]


correct_negative = [6, 7, 2, 6]
wrong_positive = [108, 97, 54, 100]
wrong_neutral = [296, 117, 196, 160]

print("Model Prediction Breakdown by Answer Type (Text Graph)\n")

for i in range(len(types)):
    label = types[i]
    correct = correct_negative[i]
    wrong_p = wrong_positive[i]
    wrong_n = wrong_neutral[i]


    scale = 0.2


    bar_correct = "█" * int(correct * scale)
    bar_wrong_p = "█" * int(wrong_p * scale)
    bar_wrong_n = "█" * int(wrong_n * scale)

    print(f"{label:<12} → {bar_correct} correctly_negative ({correct}) "
          f"{bar_wrong_p} wrongly_positive ({wrong_p}) "
          f"{bar_wrong_n} wrongly_neutral ({wrong_n})")



import matplotlib.pyplot as plt
import pandas as pd

data = {
    "Answer Type": ["Ambiguous", "Conditional", "Contrastive", "Sarcastic"],
    "Neutral (Total)": [196, 296, 160, 117],
    "Neutral → Negative (should to be)": [19, 25, 12, 16]
}


df = pd.DataFrame(data)


df["False Negative Rate (%)"] = (df["Neutral → Negative (should to be)"] / df["Neutral (Total)"]) * 100


plt.figure(figsize=(5, 4.5))
bars = plt.bar(df["Answer Type"], df["False Negative Rate (%)"], color='skyblue')


for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, height + 0.2, f"{height:.1f}%", ha='center', va='bottom')

plt.title("False Negative Rate within Neutral Predictions by Answer Type", fontsize=10)
plt.ylabel("False Negative Rate (%)", fontsize=10)
plt.ylim(1, max(df["False Negative Rate (%)"])+1)
plt.grid(axis='y', linestyle='--', alpha=0.7)



plt.show()



import matplotlib.pyplot as plt
import pandas as pd


data = {
    "Answer Type": ["Ambiguous", "Conditional", "Contrastive", "Sarcastic"],
    "Neutral Total": [196, 296, 160, 117],
    "True Negative": [19, 25, 12, 16],  # false negative
    "True Positive": [177, 271, 148, 101]  # false positive
}

df = pd.DataFrame(data)


plt.figure(figsize=(5, 4.5))

bar1 = plt.bar(df["Answer Type"], df["True Positive"], label="True Positive (→ neutral)", color="skyblue")
bar2 = plt.bar(df["Answer Type"], df["True Negative"], bottom=df["True Positive"], label="True Negative (→ neutral)", color="red")


for i in range(len(df)):
    total = df["Neutral Total"][i]
    neg = df["True Negative"][i]
    pos = df["True Positive"][i]
    plt.text(i, pos/2, f"{pos}", ha='center', va='center', fontsize=9)
    plt.text(i, pos + neg/2, f"{neg}", ha='center', va='center', fontsize=9)


plt.title("Composition of 'Neutral' Predictions by True Label and Answer Type", fontsize=14)
plt.ylabel("Number of Samples")
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

plt.show()

import matplotlib.pyplot as plt
import pandas as pd


data = {
    "Answer Type": ["Ambiguous", "Conditional", "Contrastive", "Sarcastic"],
    "Neutral Total": [196, 296, 160, 117],
    "True Negative": [19, 25, 12, 16],  # false negative
    "True Positive": [177, 271, 148, 101]  # false positive
}

df = pd.DataFrame(data)


df["Negative %"] = df["True Negative"] / df["Neutral Total"] * 100
df["Positive %"] = df["True Positive"] / df["Neutral Total"] * 100


plt.figure(figsize=(6, 5))

bar1 = plt.bar(df["Answer Type"], df["True Positive"], label="True Positive (→ neutral)", color="skyblue")
bar2 = plt.bar(df["Answer Type"], df["True Negative"], bottom=df["True Positive"], label="True Negative (→ neutral)", color="red")


for i in range(len(df)):
    pos = df["True Positive"][i]
    neg = df["True Negative"][i]
    total = df["Neutral Total"][i]


    plt.text(i, pos / 2, f"{pos}\n({df['Positive %'][i]:.1f}%)", ha='center', va='center', fontsize=9, color='black')


    plt.text(i, pos + neg / 2, f"{neg}\n({df['Negative %'][i]:.1f}%)", ha='center', va='center', fontsize=9, color='black')


plt.title("Composition of 'Neutral' Predictions by True Label and Answer Type", fontsize=14)
plt.ylabel("Number of Samples")
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

plt.show()



import pandas as pd
import matplotlib.pyplot as plt


data = {
    "Answer Type": ["Ambiguous", "Conditional", "Contrastive", "Sarcastic"],
    "Predicted Negative": [2, 6, 6, 7],
    "Predicted Positive": [18, 10, 8, 37],
    "Predicted Neutral": [19, 25, 12, 16]
}


df = pd.DataFrame(data)
df.set_index("Answer Type", inplace=True)


df["Total"] = df.sum(axis=1)


colors = {
    "Predicted Negative": "red",
    "Predicted Positive": "skyblue",
    "Predicted Neutral": "gray"
}
plot_order = ["Predicted Negative", "Predicted Positive", "Predicted Neutral"]


fig, ax = plt.subplots(figsize=(6, 5))
bottom = None

for col in plot_order:
    bar = ax.bar(df.index, df[col], bottom=bottom, label=col, color=colors[col])

    for idx, rect in enumerate(bar):
        total = df.iloc[idx]["Total"]
        height = rect.get_height()
        if height > 0:
            percent = f"{(height / total * 100):.1f}%"
            ax.text(rect.get_x() + rect.get_width() / 2, rect.get_y() + height / 2,
                    percent, ha='center', va='center', fontsize=9)
    bottom = df[col] if bottom is None else bottom + df[col]


ax.set_title("Prediction Distribution for True Negative Samples by Answer Type", fontsize=14)
ax.set_ylabel("Number of Samples")
ax.set_ylim(0, df["Total"].max() + 10)
ax.legend(title="Model Prediction")
ax.grid(axis='y', linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()



import pandas as pd
import matplotlib.pyplot as plt


data = {
    "Answer Type": ["Ambiguous", "Conditional", "Contrastive", "Sarcastic"],
    "Predicted Negative": [52, 102, 94, 90],
    "Predicted Positive": [83, 41, 78, 120],
    "Predicted Neutral": [177, 271, 148, 101]
}


df = pd.DataFrame(data)
df.set_index("Answer Type", inplace=True)


df["Total"] = df.sum(axis=1)

colors = {
    "Predicted Negative": "red",
    "Predicted Positive": "skyblue",
    "Predicted Neutral": "gray"
}
plot_order = ["Predicted Negative", "Predicted Positive", "Predicted Neutral"]


fig, ax = plt.subplots(figsize=(6, 6))
bottom = None

for col in plot_order:
    bar = ax.bar(df.index, df[col], bottom=bottom, label=col, color=colors[col])

    for idx, rect in enumerate(bar):
        total = df.iloc[idx]["Total"]
        height = rect.get_height()
        if height > 0:
            percent = f"{(height / total * 100):.1f}%"
            ax.text(rect.get_x() + rect.get_width() / 2, rect.get_y() + height / 2,
                    percent, ha='center', va='center', fontsize=9)
    bottom = df[col] if bottom is None else bottom + df[col]


ax.set_title("Prediction Distribution for True Positive Samples by Answer Type", fontsize=14)
ax.set_ylabel("Number of Samples")
ax.set_ylim(0, df["Total"].max() + 10)
ax.legend(title="Model Prediction")
ax.grid(axis='y', linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()

#new prompt.

import pandas as pd
import matplotlib.pyplot as plt


data = {
    "Answer Type": ["Ambiguous", "Conditional", "Contrastive", "Sarcastic"],
    "Predicted Negative": [4, 7, 10, 7],
    "Predicted Positive": [94, 172, 126, 103],
    "Predicted Neutral": [1, 0, 0, 0]
}


df = pd.DataFrame(data)
df.set_index("Answer Type", inplace=True)


df["Total"] = df.sum(axis=1)


colors = {
    "Predicted Negative": "orchid",
    "Predicted Positive": "indianred",
    "Predicted Neutral": "gray"
}
plot_order = ["Predicted Negative", "Predicted Positive", "Predicted Neutral"]

fig, ax = plt.subplots(figsize=(6, 5))
bottom = None

for col in plot_order:
    bar = ax.bar(df.index, df[col], bottom=bottom, label=col, color=colors[col])

    for idx, rect in enumerate(bar):
        total = df.iloc[idx]["Total"]
        height = rect.get_height()
        if height > 0:
            percent = f"{(height / total * 100):.1f}%"
            ax.text(rect.get_x() + rect.get_width() / 2, rect.get_y() + height / 2,
                    percent, ha='center', va='center', fontsize=9)
    bottom = df[col] if bottom is None else bottom + df[col]

ax.set_title("Prediction Distribution for True Negative Samples by Answer Type", fontsize=14)
ax.set_ylabel("Number of Samples")
ax.set_ylim(0, df["Total"].max() + 10)
ax.legend(title="Model Prediction")
ax.grid(axis='y', linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()



import pandas as pd
import matplotlib.pyplot as plt


data = {
    "Answer Type": ["Ambiguous", "Conditional", "Contrastive", "Sarcastic"],
    "Predicted Negative": [34, 34, 15, 53],
    "Predicted Positive": [208, 232, 186, 208],
    "Predicted Neutral": [11, 10, 9, 0]
}


df = pd.DataFrame(data)
df.set_index("Answer Type", inplace=True)


df["Total"] = df.sum(axis=1)


colors = {
    "Predicted Negative": "darkseagreen",
    "Predicted Positive": "tomato",
    "Predicted Neutral": "slategray"
}
plot_order = ["Predicted Negative", "Predicted Positive", "Predicted Neutral"]


fig, ax = plt.subplots(figsize=(6, 5))
bottom = None

for col in plot_order:
    bar = ax.bar(df.index, df[col], bottom=bottom, label=col, color=colors[col])

    for idx, rect in enumerate(bar):
        total = df.iloc[idx]["Total"]
        height = rect.get_height()
        if height > 0:
            percent = f"{(height / total * 100):.1f}%"
            ax.text(rect.get_x() + rect.get_width() / 2, rect.get_y() + height / 2,
                    percent, ha='center', va='center', fontsize=9)
    bottom = df[col] if bottom is None else bottom + df[col]

ax.set_title("Prediction Distribution for True Positive Samples by Answer Type", fontsize=14)
ax.set_ylabel("Number of Samples")
ax.set_ylim(0, df["Total"].max() + 10)
ax.legend(title="Model Prediction")
ax.grid(axis='y', linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()



#reasoning

import csv
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

model_name = "mistralai/Mistral-7B-Instruct-v0.2"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    use_auth_token=True
)
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Prompt
def build_classification_prompt(question: str, answer: str) -> str:
    return (
        "You are a pragmatic linguist. Classify the following answer into exactly one of four types:\n"
        "- ambiguous\n"
        "- conditional\n"
        "- contrastive\n"
        "- sarcastic\n"
        "Respond with ONLY the label (no explanation).\n\n"
        f"Q: {question}\n"
        f"A: {answer}\n\n"
        "Type:"
    )


input_tsv  = "LLM.tsv"
output_tsv = "mistral_four_class.tsv"


with open(input_tsv,  "r", encoding="utf-8") as fin, \
     open(output_tsv, "w", encoding="utf-8", newline="") as fout:

    reader = csv.DictReader(fin, delimiter="\t")
    fieldnames = reader.fieldnames + ["predicted_label_mistral"]
    writer = csv.DictWriter(fout, fieldnames=fieldnames, delimiter="\t")
    writer.writeheader()

    for row in reader:
        q = row["question"]
        a = row["answer"]
        prompt = build_classification_prompt(q, a)

        # zero-shot
        result = generator(prompt, max_new_tokens=5, do_sample=False)
        generated = result[0]["generated_text"]


        pred = generated.split("Type:")[-1].strip().split()[0].lower()

        if pred not in {"ambiguous", "conditional", "contrastive", "sarcastic"}:
            pred = "unclassified"

        row["predicted_label_mistral"] = pred
        writer.writerow(row)

        print(f"[{row.get('id','?')}] Mistral → {pred}")

print(f"\n✅ Done. Results saved to '{output_tsv}'")

import csv


input_tsv = "mistral_four_class.tsv"


total_mismatch = 0
positive_mismatch = 0
negative_mismatch = 0

with open(input_tsv, "r", encoding="utf-8") as fin:
    reader = csv.DictReader(fin, delimiter="\t")
    total_rows = 0

    for row in reader:
        total_rows += 1
        type_gold = row["type"].strip().lower()
        type_pred = row["predicted_label_mistral"].strip().lower()
        pred_label_1 = row["predicted_label_1"].strip().lower()

        if type_pred != type_gold:
            total_mismatch += 1
            if pred_label_1 == "positive":
                positive_mismatch += 1
            elif pred_label_1 == "negative":
                negative_mismatch += 1


print("──────────── Summary ────────────")
print(f"Total samples                       : {total_rows}")
print(f"Total mismatches (predicted_label_mistral ≠ type)      : {total_mismatch}")
print(f"→ Among them, predicted = positive : {positive_mismatch}")
print(f"→ Among them, predicted = negative : {negative_mismatch}")

import csv
from collections import defaultdict

input_tsv = "mistral_four_class.tsv"


counts_all = defaultdict(int)
correct_all = defaultdict(int)

counts_pos = defaultdict(int)
correct_pos = defaultdict(int)

counts_neg = defaultdict(int)
correct_neg = defaultdict(int)

with open(input_tsv, "r", encoding="utf-8") as fin:
    reader = csv.DictReader(fin, delimiter="\t")
    total_rows = 0

    for row in reader:
        total_rows += 1
        true_label = row["type"].strip().lower()
        pred_label = row["predicted_label_mistral"].strip().lower()
        pred_binary = row["predicted_label_1"].strip().lower()


        counts_all[true_label] += 1
        if true_label == pred_label:
            correct_all[true_label] += 1


        if pred_binary == "positive":
            counts_pos[true_label] += 1
            if true_label == pred_label:
                correct_pos[true_label] += 1


        elif pred_binary == "negative":
            counts_neg[true_label] += 1
            if true_label == pred_label:
                correct_neg[true_label] += 1

def print_accuracy(title, counts, corrects):
    print(f"\n──────────── {title} ────────────")
    total_correct = 0
    total_count = 0
    for label in counts:
        count = counts[label]
        correct = corrects.get(label, 0)
        acc = correct / count * 100 if count > 0 else 0
        print(f"{label.capitalize():<12}: {correct}/{count} → {acc:.2f}%")
        total_correct += correct
        total_count += count
    overall = total_correct / total_count * 100 if total_count > 0 else 0
    print(f"→ Overall Accuracy: {total_correct}/{total_count} → {overall:.2f}%")


print_accuracy("Accuracy on All Samples", counts_all, correct_all)
print_accuracy("Accuracy on predicted_label_1 == positive", counts_pos, correct_pos)
print_accuracy("Accuracy on predicted_label_1 == negative", counts_neg, correct_neg)

# build confusion matrix of four types

import pandas as pd

df = pd.read_csv("mistral_four_class.tsv", sep="\t")

misclassified_df = df[df["type"] != df["predicted_label_mistral"]]

confusion = pd.crosstab(misclassified_df["type"], misclassified_df["predicted_label_mistral"], rownames=["True Label"], colnames=["Predicted Label"])


print(confusion)



