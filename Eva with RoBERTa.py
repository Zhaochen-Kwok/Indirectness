# -*- coding: utf-8 -*-
"""important.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zE2ylICAJCb5rs2ZogP0KHXa9c8hSDtw
"""

!pip install --no-cache-dir transformers

import pandas as pd
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
import transformers
from tqdm import tqdm

train_df = pd.read_csv("roberta_train_split.tsv", sep="\t")
dev_df = pd.read_csv("roberta_dev.tsv", sep="\t")
test_df = pd.read_csv("4000data.tsv", sep="\t")

label2id = {label: idx for idx, label in enumerate(train_df["label"].unique())}
id2label = {idx: label for label, idx in label2id.items()}
train_df["label_id"] = train_df["label"].map(label2id)
dev_df["label_id"] = dev_df["label"].map(label2id)

dev_df["label_id"].value_counts()

class QADataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len=128, with_labels=True):
        self.data = dataframe
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.with_labels = with_labels

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        row = self.data.iloc[index]
        inputs = self.tokenizer(
            row['question'],
            row['answer'],
            padding='max_length',
            truncation=True,
            max_length=self.max_len,
            return_tensors="pt"
        )
        item = {key: val.squeeze(0) for key, val in inputs.items()}
        if self.with_labels:
            item['labels'] = torch.tensor(row['label_id'])
        return item

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
model = RobertaForSequenceClassification.from_pretrained("roberta-base", num_labels=len(label2id))

train_dataset = QADataset(train_df, tokenizer)
dev_dataset = QADataset(dev_df, tokenizer)
test_dataset = QADataset(test_df, tokenizer, with_labels=False)

training_args = TrainingArguments(
    output_dir="./results",
    do_train=True,
    do_eval=True,
    eval_strategy="steps",
    eval_steps=500,
    save_strategy="steps",
    save_steps=500,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    greater_is_better=True,
    num_train_epochs=10,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    logging_dir="./logs",
    logging_steps=50,
    overwrite_output_dir=True,
    report_to=[]
)

# Trainer
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from transformers import EvalPrediction

def compute_metrics(p: EvalPrediction):
    preds = p.predictions.argmax(-1)
    labels = p.label_ids
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    acc = accuracy_score(labels, preds)
    return {
        "accuracy": acc,
        "f1": f1,
        "precision": precision,
        "recall": recall
    }

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=dev_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

trainer.evaluate()

from transformers import RobertaTokenizer, RobertaForSequenceClassification

tokenizer = RobertaTokenizer.from_pretrained("saved_roberta_model")
model = RobertaForSequenceClassification.from_pretrained("saved_roberta_model")
model.eval()

test_df

test_loader = DataLoader(test_dataset, batch_size=8)
model.eval()

pd.Series(preds).value_counts()

import torch
from tqdm import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

preds = []

with torch.no_grad():
    for batch in tqdm(test_loader):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        preds.extend(torch.argmax(logits, dim=1).tolist())

test_df["predicted_label"] = [id2label[p] for p in preds]
test_df.to_csv("test_with_predictions.tsv", sep="\t", index=False)

import pandas as pd

test_df = pd.read_csv("1.tsv", sep="\t")

num_wrong = (test_df["predicted_label"] != test_df["Label"]).sum()
print(f"The number of wrong predictions：{num_wrong}")

# true label is YES
wrong_yes = test_df[(test_df["Label"] == "Yes") & (test_df["predicted_label"] != "Yes")]


num_wrong_yes = len(wrong_yes)
print(f'The number of true label is YES but pred.NO：{num_wrong_yes}')

# True label is NO:
wrong_yes = test_df[(test_df["Label"] == "No") & (test_df["predicted_label"] != "No")]


num_wrong_yes = len(wrong_yes)
print(f'The number of true label is NO but pred.YES ：{num_wrong_yes}')
