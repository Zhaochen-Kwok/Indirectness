# -*- coding: utf-8 -*-
"""Filtering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19gGoRLueX9R1BI306Rw_ZftQvKRxqwjn
"""

!pip install transformers torch huggingface_hub

from transformers import AutoModelForCausalLM, AutoTokenizer
from huggingface_hub import notebook_login
import torch

notebook_login()

#filtering
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# ===== Step 1:  Gemma model =====
model_name = "google/gemma-2b-it"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# ===== Step 2: read dataÔºàincluding id, question, answer, typeÔºâ=====
input_file = 'yes.tsv'#"Indirect-NO.tsv"
df = pd.read_csv(input_file, sep="\t")
assert all(col in df.columns for col in ["id", "question", "answer", "type"]), ".TSV should contain id, question, answer, type."

! pip install tqdm

from tqdm import tqdm
import pandas as pd

# ===== Step 3: Prompt =====
def build_prompt(question, answer):
    return f"""<start_of_turn>user
Read the following question and answer.
Decide whether the answer expresses a positive or negative attitude.
Respond with only "positive" or "negative".

Question: {question}
Answer: {answer}
<end_of_turn>
<start_of_turn>model
"""

# ===== Step 4: classification =====
def classify_attitude(prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=5)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    pred = response.split("model")[-1].strip().lower()
    if "positive" in pred:
        return "positive"
    elif "negative" in pred:
        return "negative"
    else:
        return "unknown"

# ===== Step 5:  =====
results = []
errors = []

for i, (_, row) in enumerate(tqdm(df.iterrows(), total=len(df), desc="Classifying")):
    try:
        prompt = build_prompt(row["question"], row["answer"])
        pred = classify_attitude(prompt)

        results.append({
            "id": row["id"],
            "question": row["question"],
            "answer": row["answer"],
            "type": row["type"],
            "predicted_label": pred
        })

        # print examples
        if i < 3:
            print(f"\nüìå Example {i} | ID: {row['id']}")
            print(prompt)
            print(f"‚Üí Prediction: {pred}")

        # =====  =====
        if i > 0 and i % 100 == 0:
            pd.DataFrame(results).to_csv("gemma_predictions_partial.tsv", sep="\t", index=False)
            print(f"üìù Autosaved {len(results)} predictions to gemma_predictions_partial.tsv at step {i}")

    except Exception as e:
        print(f"‚ùå Error on row {i} (ID: {row['id']}): {e}")
        errors.append({
            "id": row["id"],
            "error": str(e)
        })

input_file = 'yes.tsv'#"Indirect-NO.tsv"
df = pd.read_csv(input_file, sep="\t")
assert all(col in df.columns for col in ["id", "question", "answer", "type"]), ".TSV should contain id, question, answer, type."

def build_prompt(question, answer):
    return f"""<start_of_turn>user
Read the following question and answer. Decide whether the answer expresses a positive or negative attitude. Respond with only "positive" or "negative".
Question: {question}
Answer: {answer}
<end_of_turn>
<start_of_turn>model
"""


def classify_attitude(prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=5)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    pred = response.split("model")[-1].strip().lower()
    if "positive" in pred:
        return "positive"
    elif "negative" in pred:
        return "negative"
    else:
        return "unknown"

results = []
for _, row in df.iterrows():
    prompt = build_prompt(row["question"], row["answer"])
    pred = classify_attitude(prompt)
    results.append({
        "id": row["id"],
        "question": row["question"],
        "answer": row["answer"],
        "type": row["type"],
        "predicted_label": pred
    })


output_df = pd.DataFrame(results)
output_df.to_csv("classified_with_type.tsv", sep="\t", index=False)

print(f"total data {len(df)}Ôºåsaved to classified_with_type.tsv")